{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Library"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import platform\n","import os\n","import re\n","import random\n","import multiprocessing\n","import itertools\n","from datetime import datetime\n","from collections import Counter\n","\n","import nltk\n","import jieba\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n",""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["SEED = 42\n","\n","os.environ['PYTHONHASHSEED']=str(SEED)\n","random.seed(SEED)\n","np.random.seed(SEED)\n",""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["MAX_WORD = 200\n","EMBEDDING_DIMENSION = 30\n",""]},{"cell_type":"code","execution_count":4,"metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":"Python version: 3.8.3\nJieba Version: 0.42.1\nNLTK Version: 3.5\nPandas Version: 1.0.5\nNumpy Version: 1.19.0\nSeaborn Version: 0.10.1\n"}],"source":["print('Python version:', platform.python_version())\n","print('Jieba Version:', jieba.__version__)\n","print('NLTK Version:', nltk.__version__)\n","print('Pandas Version:', pd.__version__)\n","print('Numpy Version:', np.__version__)\n","print('Seaborn Version:', sns.__version__)\n","# print('FastText Version:', fasttext.__version__)\n",""]},{"cell_type":"code","execution_count":5,"metadata":{"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":"Building prefix dict from the default dictionary ...\nLoading model from cache /tmp/jieba.cache\nLoading model cost 0.549 seconds.\nPrefix dict has been built successfully.\n"}],"source":["jieba.enable_parallel(multiprocessing.cpu_count())\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["train_en = pd.read_csv('./data/csv/train_en.csv', usecols=['product_title'])\n","train_en = train_en['product_title']\n","\n","train_tcn = pd.read_csv('./data/csv/train_tcn.csv', usecols=['product_title'])\n","train_tcn = train_tcn['product_title']\n","\n","dev_en = pd.read_csv('./data/csv/dev_en.csv', usecols=['translation_output'])\n","dev_en = dev_en['translation_output']\n","\n","dev_tcn = pd.read_csv('./data/csv/dev_tcn.csv', usecols=['text'])\n","dev_tcn = dev_tcn['text']\n","\n","test_tcn = pd.read_csv('./data/csv/test_tcn.csv', usecols=['text'])\n","test_tcn = test_tcn['text']\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Clean Text\n","\n"," 1. Lower case all title\n"," 2. Remove title containing '\\n', '\\\"' or ','\n"," 3. Tokenize and\n","   * Only keep alphabet for english title\n","   * Only keep CJK Unified Ideographs for traditional chinese title\n"," 4. Join token into string with whitespace as seperator"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def clean_tcn(text):\n","    try:\n","        # 4E00â€”9FFF is range of CJK (Chinese, Japanese, Korean) Unified Ideographs\n","        # https://unicode-table.com/en/blocks/cjk-unified-ideographs/\n","        new_text = re.sub(r'[^\\u4e00-\\u9fff]', ' ', text)\n","        return new_text\n","    except Exception as ex:\n","        print(f'Text : {text}')\n","        print(ex)\n","        return text\n","\n","def tokenize_tcn(text, mode):\n","    try:\n","        result = jieba.tokenize(text, mode=mode)\n","        token = [r[0] for r in result]\n","    except Exception as ex:\n","        print(f'Text : {text}')\n","        print(ex)\n","        token = []\n","    finally:\n","        return token\n","\n","def tokenize_en(text):\n","    try:\n","        token = nltk.word_tokenize(text)\n","        token = [t for t in token if t == re.sub(r'[^a-z]', '', t)]\n","        return token\n","    except:\n","        return []\n","\n","def token2text(token):\n","    try:\n","        text = ' '.join(token)\n","        return text\n","    except Exception as ex:\n","        print(f'Token : {token}')\n","        print(ex)\n","        return token\n","\n","def preprocess(sr, lang='en', mode='default'):\n","    sr = sr.str.lower()\n","    sr = sr[~((sr.str.contains('\\n')) & \n","                (sr.str.contains('\\\"')) & \n","                (sr.str.contains(',')))]\n","    \n","    if lang == 'en':\n","        sr = sr.apply(tokenize_en)\n","    else: # tcn\n","        sr = sr.apply(clean_tcn)\n","        sr = sr.apply(lambda t: tokenize_tcn(t, mode))\n","    sr = sr.apply(token2text)\n","\n","    return sr\n",""]},{"cell_type":"code","execution_count":8,"metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":"Text : nan\nexpected string or bytes-like object\nText : nan\njieba: the input parameter should be unicode.\n"}],"source":["train_en = preprocess(train_en)\n","train_tcn = preprocess(train_tcn, lang='tcn', mode='default')\n","train_tcn2 = preprocess(train_tcn, lang='tcn', mode='search')\n","val_en = preprocess(train_en)\n","val_tcn = preprocess(train_tcn, lang='tcn', mode='default')\n","val_tcn2 = preprocess(train_tcn, lang='tcn', mode='search')\n","test_tcn = preprocess(train_tcn, lang='tcn', mode='search')\n","test_tcn2 = preprocess(train_tcn, lang='tcn', mode='search')\n",""]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["en_sr = pd.concat([train_en, val_en], axis=0)\n","en_sr = en_sr.dropna().drop_duplicates(keep='first').reset_index(drop=True)\n","tcn_sr = pd.concat([train_tcn, train_tcn2, val_tcn, val_tcn2, test_tcn, test_tcn2], axis=0)\n","tcn_sr = tcn_sr.dropna().drop_duplicates(keep='first').reset_index(drop=True)\n",""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["tcn_sr = tcn_sr.apply(lambda t: re.sub(r'\\s{2,}', ' ', t))\n","tcn_sr = tcn_sr.str.lstrip(' ')\n","tcn_sr = tcn_sr.str.rstrip(' ')\n",""]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["en_sr.to_csv('./data/txt/train_en.txt', header=False, index=False)\n","tcn_sr.to_csv('./data/txt/train_tcn.txt', header=False, index=False)\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Library"]},{"cell_type":"code","execution_count":1,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"}],"source":["import platform\n","import multiprocessing\n","import itertools\n","import re\n","\n","import pandas as pd\n","import textattack\n","import textda\n","import jieba\n",""]},{"cell_type":"code","execution_count":2,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Python version: 3.8.3\nPandas version: 1.0.5\nTextAttack version: 0.2.2\ntextda version: 0.1.0.6\nJieba version: 0.42.1\n"}],"source":["print('Python version:', platform.python_version())\n","print('Pandas version:', pd.__version__)\n","# print('TextAttack version:', textattack.__version__)\n","print('TextAttack version: 0.2.2')\n","# print('textda version:', textda.__version__)\n","print('textda version: 0.1.0.6')\n","print('Jieba version:', jieba.__version__)\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["df_tcn = pd.read_csv('./data/txt/train_tcn.txt', header=None).dropna()\n","df_en = pd.read_csv('./data/txt/train_en.txt', header=None).dropna()\n","df_en[0] = df_en[0].apply(lambda t: re.sub('[^a-z ]', '', t))\n",""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["list_tcn = df_tcn[0].to_list()\n","list_en = df_en[0].to_list()\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Augment"]},{"cell_type":"code","execution_count":5,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stderr","text":"Building prefix dict from /home/ilos-vigil/.local/lib/python3.8/site-packages/synonyms/data/vocab.txt ...\nBuilding prefix dict from /home/ilos-vigil/.local/lib/python3.8/site-packages/synonyms/data/vocab.txt ...\n>> Synonyms load wordseg dict [/home/ilos-vigil/.local/lib/python3.8/site-packages/synonyms/data/vocab.txt] ... \nDumping model to file cache /tmp/jieba.u8c5535dea22e9d74e4ba244d4b27c629.cache\nDumping model to file cache /tmp/jieba.u8c5535dea22e9d74e4ba244d4b27c629.cache\nLoading model cost 1.567 seconds.\nLoading model cost 1.567 seconds.\nPrefix dict has been built successfully.\nPrefix dict has been built successfully.\n>> Synonyms on loading stopwords [/home/ilos-vigil/.local/lib/python3.8/site-packages/synonyms/data/stopwords.txt] ...\n>> Synonyms on loading vectors [/home/ilos-vigil/.local/lib/python3.8/site-packages/synonyms/data/words.vector] ...\n"}],"source":["from textda.data_expansion import data_expansion\n","wn_augmenter = textattack.augmentation.WordNetAugmenter()\n",""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def flatten_2d(old_list):\n","    new_list = []\n","    for sublist in old_list:\n","        for item in sublist:\n","            new_list.append(item)\n","    return new_list\n","\n","def split_tcn(text, mode='default'):\n","    try:\n","        result = jieba.tokenize(text, mode=mode)\n","        token = [r[0] for r in result]\n","        new_text = ' '.join(token)\n","        return new_text\n","    except Exception as ex:\n","        print(f'Text : {text}')\n","        print(ex)\n","        return text\n","\n","def aug_tcn(sentence):\n","    try:\n","        augmented_sentences = data_expansion(sentence, alpha_sr=1.0, alpha_ri=0, alpha_rs=0, p_rd=0, num_aug=10)\n","\n","        augmented_tokenized_sentences = []\n","        for s in augmented_sentences:\n","            augmented_tokenized_sentences.append(split_tcn(s, mode='default'))\n","            augmented_tokenized_sentences.append(split_tcn(s, mode='search'))\n","\n","        return augmented_tokenized_sentences\n","    except:\n","        return [sentence]\n","\n","def aug_en(sentence):\n","    sentence = str(sentence)\n","    sentence = re.sub('[^a-z ]', '', sentence)\n","    augmented_sentences = [[sentence]]\n","    try:\n","        if ' ' in sentence:\n","            for _ in range(10):\n","                augmented_sentence = wn_augmenter.augment(sentence)\n","                augmented_sentences.append(augmented_sentence)\n","    except Exception as ex:\n","        print(f'Sentence: {sentence} |\\n')\n","\n","\n","    augmented_sentences = list(itertools.chain.from_iterable(augmented_sentences))\n","    augmented_sentences = list(set(augmented_sentences))\n","    augmented_sentences = [s.lower() for s in augmented_sentences]\n","    return augmented_sentences\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["list_aug_en = []\n","with multiprocessing.Pool(multiprocessing.cpu_count() - 1) as pool:\n","    list_aug_en = pool.map(aug_en, list_en)\n","    list_aug_en = list(itertools.chain.from_iterable(list_aug_en))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_aug_en = pd.DataFrame({'en': list_aug_en})\n","df_aug_en.to_csv('./data/txt/train_en_aug.txt', index=False, header=False)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["list_aug_tcn = []\n","\n","with multiprocessing.Pool(multiprocessing.cpu_count() - 1) as pool:\n","    list_aug_tcn = pool.map(aug_tcn, list_tcn)\n","    list_aug_tcn = flatten_2d(list_aug_tcn)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_aug_tcn = pd.DataFrame({'tcn': list_aug_tcn})\n","df_aug_tcn.to_csv('./data/txt/train_tcn_aug.txt', index=False, header=False)\n","\n",""]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3-final"},"orig_nbformat":2,"kernelspec":{"name":"python37664bit20d3e4822ada4780908611045980e7a0","display_name":"Python 3.7.6 64-bit"}},"nbformat":4,"nbformat_minor":2}